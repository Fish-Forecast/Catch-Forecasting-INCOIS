<!DOCTYPE html>
<html>
  <head>
    <title></title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: center, middle, inverse
# Forecasting Time Series
## Eli Holmes
.large[_Northwest Fisheries Science Center&lt;br&gt;National Oceanic and Atmospheric Administration&lt;br&gt;Seattle, WA_]

.futnote[eli.holmes@noaa.gov]

.citation[@eli_holmes]

---

## Forecasting Time Series

This week we will learn a number of standard approaches for forecasting from time series alone (meaning without any covariates or exogenous variables).  In later, weeks we will address how to incorporate covariates into a time series model and forecast.

---

## Many approaches are available for non-seasonal modeling

.pull-left.left[
*Stergiou and Christou 1995*

- Time-varying regression
- Box-Jenkins models, aka ARIMA models
- Multivariate time-series approaches
    - Harmonic regression
    - Dynamic regression
    - Vector autoregression (MAR)
- Exponential smoothing (2 variants)
- Exponential surplus yield model (FOX)
]

.pull-right.left[
*Georgakarakos et al. 2006*

- Box-Jenkins models, aka ARIMA models
- Artificial neural networks (ANNs)
- Bayesian dynamic models

*Lawer 2016*

- Box-Jenkins models, aka ARIMA models
- Artificial neural networks (ANNs)
- Exponential Smoothing (6 variants)
]

---

## Forecasting Time Series

We will focus on a subset of these methods.  There are methods have a very long tradition (ARIMA), are often found to be the best or equally best method (Exp smoothing), are non-parametric and recently used in a number of fisheries papers (EDM), and are commonly used in many variants (DLM):

- Time-varying regression
- Box-Jenkins models, aka ARIMA models
- Exponential Smoothing
- Empirical dynamic modeling (EDM)
- Dynamic linear modeling (DLM)

---

## Data

We will use the annual landings data from Hellenic (Greek) waters that were used in Stergiou and Christou (1995). Stergiou and Christou analyzed 16 species.  We will look a subset of species: Anchovy, Sardine, Chub mackerel, Horse mackerel, and Jack mackerel.  Stergiou and Christou used the data from 1964-1989.  We have the data up to 2007, but will focus mainly on 1964-1989 (the first half of the time series) to replicate Stergiou and Christou's analyses.

---

Load the data as follows and use only the 1964-1989 data.

```r
load("landings.RData")
landings$log.metric.tons = log(landings$metric.tons)
landings = subset(landings, Year &lt;= 1989)
library(ggplot2)
ggplot(landings, aes(x=Year, y=metric.tons)) +
  geom_line() + facet_wrap(~Species)
```

&lt;img src="Forecasting_1_files/figure-html/load_data-1.png" style="display: block; margin: auto;" /&gt;

---

## Time-varying regression

Time-varying regression is simply a linear regression where time is the explanatory variable:

`$$log(catch) = \alpha + \beta t + \beta_2 t^2 + \dots + e_t$$`
The error term ( `\(e_t\)` ) was treated as an independent Normal error ( `\(\sim N(0, \sigma)\)` ) in Stergiou and Christou (1995).  If that is not a reasonable assumption, then it is simple to fit an autocorrelated error model or non-Gausian error model in R.

---

Stergiou and Christou (1995) fit time-varying regressions to the 1964-1987 data and show the results in Table 4.

![Table 4](./figs/SC1995Table4.png)

---

The first step is to determine how many polynomials of `\(t\)` to include in your model.

&lt;img src="Forecasting_1_files/figure-html/poly.plot-1.png" style="display: block; margin: auto;" /&gt;

---

Here is how to fit a linear regression to the anchovy landings with a 4th-order polynomial for time.  We are fitting this model:

`$$log(Anchovy) = \alpha + \beta t + \beta_2 t^2 + \beta_3 t^3 + \beta_4 t^4 + e_t$$`


```r
landings$t = landings$Year-1963
model &lt;- lm(log.metric.tons ~ poly(t,4), 
            data=landings, subset=Species=="Anchovy"&amp;Year&lt;=1987)
```

---

They do not say how they choose the polynomial order to include.  We will look at the fit and keep the significant polynomials.


```r
summary(model)
```

```
## 
## Call:
## lm(formula = log.metric.tons ~ poly(t, 4), data = landings, subset = Species == 
##     "Anchovy" &amp; Year &lt;= 1987)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.26951 -0.09922 -0.01018  0.11777  0.20006 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  9.17747    0.03541 259.213  &lt; 2e-16 ***
## poly(t, 4)1  6.06211    0.55180  10.986 1.13e-09 ***
## poly(t, 4)2  1.77153    0.59015   3.002  0.00733 ** 
## poly(t, 4)3  0.68734    0.57016   1.206  0.24280    
## poly(t, 4)4 -0.49989    0.51403  -0.972  0.34302    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.1458 on 19 degrees of freedom
## Multiple R-squared:  0.9143,	Adjusted R-squared:  0.8962 
## F-statistic: 50.65 on 4 and 19 DF,  p-value: 7.096e-10
```

---

This suggests that we keep only the 1st polynomial, i.e. a linear relationship with time.


```r
dat = subset(landings, Species=="Anchovy" &amp; Year &lt;= 1987)
model &lt;- lm(log.metric.tons ~ t, data=dat)
```

The coefficients and adjusted R2 are similar to that shown in Table 4.


```r
c(coef(model), summary(model)$adj.r.squared)
```

```
## (Intercept)           t             
##  8.36143085  0.05818942  0.81856644
```

---

We want to test if our residuals are independent.  We can do this with the Ljung-Box test as Stergio and Christou (1995) do.  Stergio and Christou appear to use a lag of 14 for the test (this is a bit large for 24 data points).  The degrees of freedom is lag minus the number of estimated parameters in the model.  So for the Anchovy data, `\(df = 14 - 2\)`.


```r
x &lt;- resid(model)
Box.test(x, lag = 14, type = "Ljung-Box",fitdf=5)
```

```
## 
## 	Box-Ljung test
## 
## data:  x
## X-squared = 27.18, df = 9, p-value = 0.001306
```
Compare to the values in the far right column in Table 4.

---

For the sardine (bottom row in Table 4), Stergio and Christou fit a 4th order polynomial.  There are two approaches you can take to fitting n-order polynomials.  The first is to use the `poly()` function.  This creates orthogonal covariates for your polynomial.

What does that mean? Let's say you want to fit a model with a 2nd order polynomial of `\(t\)`.  It has `\(t\)` and `\(t^2\)`, but using these are highly correlated.  They also have different means and different variances, which makes it hard to compare the estimated effect sizes.  The `poly()` function creates covariates with mean and covariance or zero and identical variances.


```r
T1 = 1:24; T2=T1^2
c(mean(T1),mean(T2),cov(T1, T2))
```

```
## [1]   12.5000  204.1667 1250.0000
```

```r
T1 = poly(T1,2)[,1]; T2=poly(T1,2)[,2]
c(mean(T1),mean(T2),cov(T1, T2))
```

```
## [1]  4.921826e-18  2.674139e-17 -4.949619e-20
```

---

With `poly()`, a 4th order time-varying regression model is fit to the sardine data as:


```r
dat = subset(landings, Species=="Sardine" &amp; Year &lt;= 1987)
model &lt;- lm(log.metric.tons ~ poly(t,4), data=dat)
```

This indicates support for the 2nd, 3rd, and 4th orders but not the 1st (linear) part:


```r
summary(model)
```

```
## 
## Call:
## lm(formula = log.metric.tons ~ poly(t, 4), data = dat)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.115300 -0.053090 -0.008895  0.041783  0.165885 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  9.31524    0.01717 542.470  &lt; 2e-16 ***
## poly(t, 4)1  0.08314    0.08412   0.988 0.335453    
## poly(t, 4)2 -0.18809    0.08412  -2.236 0.037559 *  
## poly(t, 4)3 -0.35504    0.08412  -4.220 0.000463 ***
## poly(t, 4)4  0.25674    0.08412   3.052 0.006562 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.08412 on 19 degrees of freedom
## Multiple R-squared:  0.6353,	Adjusted R-squared:  0.5586 
## F-statistic: 8.275 on 4 and 19 DF,  p-value: 0.0004846
```

----

However, Stergiou and Christou used a raw polynomial model using `\(t\)`, `\(t^2\)`, `\(t^3\)` and `\(t^4\)` as the covariates.  We can fit this model as:


```r
dat = subset(landings, Species=="Sardine" &amp; Year &lt;= 1987)
model &lt;- lm(log.metric.tons ~ t + I(t^2) + I(t^3) + I(t^4), data=dat)
```

The coefficients and adjusted R2 are similar to that shown in Table 4.


```r
c(coef(model), summary(model)$adj.r.squared)
```

```
##   (Intercept)             t        I(t^2)        I(t^3)        I(t^4) 
##  9.672783e+00 -2.443273e-01  3.738773e-02 -1.983588e-03  3.405533e-05 
##               
##  5.585532e-01
```

---

The test for autocorrelation of the residuals is 


```r
x &lt;- resid(model)
Box.test(x, lag = 14, type = "Ljung-Box",fitdf=5)
```

```
## 
## 	Box-Ljung test
## 
## data:  x
## X-squared = 32.317, df = 9, p-value = 0.0001755
```

`fitdf` specifies the number of parameters estimated by the model.  In this case it is 5, intercept and 4 coefficients.

The p-value is less than 0.05 indicating lack of autocorrelation in the residuals.

---

## ARIMA models (Box-Jenkins models)

You will commonly see ARIMA models referred to as *Box-Jenkins* models.  This model has 3 components (p, d, q):

- AR autoregressive.  `\(y_t\)` depends on past values. The AR level is maximum lag `\(p\)`.

`$$x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + ... + \phi_p x_{t-p} + e_t$$`

- I differencing. `\(x_t\)` may be a difference of the observed time series.  The number of differences is denoted `\(d\)`. First difference is `\(d=1\)`:

`$$x_t = y_t - y_{t-1}$$`

- MA moving average.  The error `\(e_t\)` can be a sum of a time series of independent random errors.  The maximum lag is denoted `\(q\)`.

`$$e_t = \eta_t + \theta_1 \eta_{t-1} + \theta_2 \eta_{t-2} + ... + \theta_q \eta_{t-q},\quad \eta_t \sim N(0, \sigma)$$`

---

## Box-Jenkins Method

This refers to a step-by-step process of selecting a forecasting model.  R will automate much of this, but you need to go through the steps otherwise you could end up fitting a nonsensical model or using fitting a sensible model with an algorithm that will not work on your data.

A. Model form selection
    1. Evaluate stationarity and seasonality
    2. Selection of the differencing level (d)
    3. Selection of the AR level (p)
    4. Selection of the MA level (q)
    
B. Parameter estimation

C. Model checking

---

## Stationarity

Stationarity means 'not changing in time' in the context of time-series models.  Typically we test the trend and variance, however more generally all statistical properties of a time-series is time-constant if the time series is 'stationary'.

Many ARMA models exhibit stationarity.  White noise is one type:
`$$x_t = e_t, e_t \sim N(0,\sigma)$$`

&lt;img src="Forecasting_1_files/figure-html/fig.stationarity-1.png" style="display: block; margin: auto;" /&gt;

---

An AR-1 process with `\(b&lt;1\)`
`$$x_t = b x_{t-1} + e_t$$`
is also stationary.

&lt;img src="Forecasting_1_files/figure-html/fig.stationarity2-1.png" style="display: block; margin: auto;" /&gt;
---

The processes shown have mean 0 and a flat level.  We can also have stationarity around an non-zero level or stationarity around an linear trend. If `\(b=0\)`, we have white noise and if `\(b&lt;1\)` we have AR-1.

1. Non-zero mean: `\(x_t = \mu + b x_{t-1} + e_t\)`

2. Linear trend: `\(x_t = \mu + at + b x_{t-1} + e_t\)`

&lt;img src="Forecasting_1_files/figure-html/fig.stationarity3-1.png" style="display: block; margin: auto;" /&gt;

---

## Non-stationarity

One of the most common forms of non-stationarity that is tested for is 'unit root', which means that the process is a random walk:
`$$x_t = x_{t-1} + e_t$$` .

&lt;img src="Forecasting_1_files/figure-html/fig.nonstationarity-1.png" style="display: block; margin: auto;" /&gt;

---

Similar to the way we added an intecept and linear trend to the stationarity processes, we can do the same to the random walk.

1. Non-zero mean or intercept: `\(x_t = \mu + x_{t-1} + e_t\)`

2. Linear trend: `\(x_t = \mu + at + x_{t-1} + e_t\)`

The effects are fundamentally different however.  The addition of `\(\mu\)` leads to a upward mean linear trend while the addition of `\(at\)` leads to exponential growth.

&lt;img src="Forecasting_1_files/figure-html/fig.stationarity4-1.png" style="display: block; margin: auto;" /&gt;

---

## Detecting stationarity

Why is evaluating stationarity important? Many AR models have a flat level or trend and time-constant variance.  If your data do not have those properties, you are fitting a model that is fundamentally inconsistent with your data.  In addition, many standard algorithms for fitting ARIMA models assume stationarity.  Note, you can fit ARIMA models without making this assumption, but you need to use the appropriate algorithm.

We will discuss three common approaches to evaluating stationarity:

- Visual test
- (Augmented) Dickey-Fuller test
- KPSS test

---

## Visual test

The visual test is simply looking at a plot of the data versus time.  Look for

- Change in the level over time.  Is the time series increasing or decreasing?  Does it appear to cycle?
- Change in the variance over time.  Do deviations away from the mean change over time, increase or decrease?

Note, if you are using an augmented Dickey-Fuller test, you may inadvertantly invalidate your test by looking at the data.

---

Here is a plot of the anchovy and sardine in Greek waters from 1965 to 1989.  The anchovies have an obvious non-stationary trend during this period.  The mean level is going up.  The sardines have a roughly stationary trend.  The variance (deviations away from the mean) appear to be roughly stationary, neither increasing or decreasing in time.

&lt;img src="Forecasting_1_files/figure-html/fig.vis-1.png" style="display: block; margin: auto;" /&gt;

Although the logged anchovy time series is increasing, it appears to have an linear trend.

---

## Dickey-Fuller test

The Dickey=Fuller test (and Augmented Dickey-Fuller test) look for evidence that the time series has a unit root.  The null hypothesis is that the time series has a unit root, that is, it has a random walk component.  The alternative hypothesis is some variation of stationarity.  The test has three main verisons. 

---

Visually, the null and alternative hypotheses for the three Dickey-Fuller tests are the following.  It is hard to see but in the panels on the left, the variance around the trend is increasing and on the right, it is not.

&lt;img src="Forecasting_1_files/figure-html/fig.df-1.png" style="display: block; margin: auto;" /&gt;

---

In math, here are the null and alternative hypotheses.  In each, we are testing if `\(\delta=0\)`.

1. Null is a random walk with no drift
`\(x_t = x_{t-1}+e_t\)`

Alternative is a mean-reverting (stationary) process with zero mean.
`\(x_t = \delta x_{t-1}+e_t\)`

2. Null is a random walk with drift (linear STOCHASTIC trend)
`\(x_t = \mu + x_{t-1} + e_t\)`

Alternative is a mean-reverting (stationary) process with non-zero mean and no trend.
`\(x_t = \mu + \delta x_{t-1} + e_t\)`

2. Null is a random walk with exponential trend
`\(x_t = \mu + at + x_{t-1} + e_t\)`

Alternative is a mean-reverting (stationary) process with non-zero mean and linear DETERMINISTIC trend.
`\(x_t = \mu + at + \delta x_{t-1} + e_t\)`

---



## Example: Dickey-Fuller tests on the Anchovy and Sardine time series


```r
require(tseries)
anchovy = subset(dat, Species=="Anchovy")$log.metric.tons
sardine = subset(dat, Species=="Sardine")$log.metric.tons
```

The `urca` R package can be used to apply the Dickey-Fuller tests.  

```
ur.df(y, type = c("none", "drift", "trend"), lags = 0, selectlags = c("Fixed", "AIC", "BIC"))
```


```r
library(urca)
ur.df(anchovy, type="none", lags=0)
```

```
## 
## ############################################################### 
## # Augmented Dickey-Fuller Test Unit Root / Cointegration Test # 
## ############################################################### 
## 
## The value of the test statistic is: 0.8195
```

The p-value can be looked up using `summary(test)`.  If the p-value is less than `\(\alpha\)`=0.05, it means the null hypothesis is rejected and the time series is non-stationary.

---

## Augmented Dickey-Fuller test

The Dickey-Fuller test assumes that the stationary process is AR-1 (autoregressive lag-1).  The Augmented Dickey-Fuller test allows a general stationary process.  The idea of the test however is the same.

We can apply the Augmented Dickey-Fuller test with the `ur.df()` function or the `adf.test()` function in the tseries package.

```
adf.test(x, alternative = c("stationary", "explosive"),
         k = trunc((length(x)-1)^(1/3)))
```

The alternative is either stationary like `\(y_t = b y_{t-1} + \eta_t\)` with `\(b&lt;1\)` or 'explosive' with `\(b&gt;1\)`.  `k` is the number of lags which determines the time-lags allowed in the autoregressions.  `k` is generally determined by the length of your time series.

## Example: Augmented Dickey-Fuller tests


```r
library(urca)
adf.test(anchovy)
```

```
## 
## 	Augmented Dickey-Fuller Test
## 
## data:  anchovy
## Dickey-Fuller = -1.6851, Lag order = 2, p-value = 0.6923
## alternative hypothesis: stationary
```

```r
adf.test(sardine)
```

```
## 
## 	Augmented Dickey-Fuller Test
## 
## data:  sardine
## Dickey-Fuller = -1.3581, Lag order = 2, p-value = 0.8169
## alternative hypothesis: stationary
```

In both cases, we do not reject the null hypothesis that the data have a random walk.  Thus there is not support for these time-series being stationary.

---

## KPSS test

In the Dickey-Fuller test, the null hypothesis is the unit root, i.e. random walk.  Often times, there is not enough power to reject the null hypothesis.  A null hypothesis is accepted unless there is strong evidence against it.  The Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test has as the null hypothesis that a time series is stationary around a level trend (or a linear trend). The alternative hypothesis for the KPSS test is a random walk.

The stationarity assumption is general; it does not assume a specific type of stationarity such as white noise.

The KPSS tests tends to reject the null (support non-stationarity) more often than it should.  So it is typically combined with the Augmented Dickey-Fuller (ADF) test.  If both tests support non-stationarity, then the stationarity assumption is not supported.

---

## Example: KPSS tests


```r
library(tseries)
kpss.test(anchovy, null="Trend")
```

```
## 
## 	KPSS Test for Trend Stationarity
## 
## data:  anchovy
## KPSS Trend = 0.1646, Truncation lag parameter = 1, p-value =
## 0.0345
```

```r
kpss.test(sardine)
```

```
## 
## 	KPSS Test for Level Stationarity
## 
## data:  sardine
## KPSS Level = 0.1844, Truncation lag parameter = 1, p-value = 0.1
```

Here `null="Trend"` was included to account for the increasing trend in the anchovy data.  For the sardine, stationarity is not rejected while for the anchovies it is.

---

What you really want to know is how can I transform my data to make it stationary?  To do that, we can normally difference our data.  The `ndiffs()` function in the forecast package makes this easy.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
